{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to KenLM\n",
    "\n",
    "\n",
    "Yes, I'm familiar with KenLM, which is an efficient and widely-used statistical language model (LM) that supports large-scale text processing. KenLM is designed to generate and query probabilistic models for sequences of words, and it's often used for speech recognition, machine translation, and natural language processing (NLP) tasks.\n",
    "\n",
    "Here are some key details about KenLM:\n",
    "\n",
    "- Efficient N-gram Language Modeling: KenLM is mainly based on n-grams, where the probability of a word is predicted based on the previous n−1n−1 words. This makes it more efficient than some other methods for certain NLP tasks.\n",
    "\n",
    "- Memory Optimization: It is highly optimized for memory usage and speed, making it useful for large-scale applications where the model size could be several gigabytes. It can work with billions of words and huge datasets without consuming too much memory.\n",
    "\n",
    "- C++ Implementation: KenLM is implemented in C++, which contributes to its speed and efficiency. It also offers Python bindings for easier integration in Python-based NLP pipelines.\n",
    "\n",
    "- Probabilistic Querying: KenLM allows querying the probability of sequences of words, which is critical in applications like decoding speech recognition hypotheses or ranking candidate translations in machine translation systems.\n",
    "\n",
    "- Arpa and Binary Formats: KenLM works with both ARPA format (text-based) and binary format for faster querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The first step will be to build KenLM. Then, we will build the ARPA file which KenLM uses to evaluate.\n",
    "Building KenLM\n",
    "\n",
    "First, clone this repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'kenlm'...\n",
      "The authenticity of host 'github.com (20.205.243.166)' can't be established.\n",
      "ED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU.\n",
      "This key is not known by any other names.\n",
      "Are you sure you want to continue connecting (yes/no/[fingerprint])? ^C\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:kpu/kenlm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo snap install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/levi/Speech-Course-Lab/Lab/Lab04 - Whisper and KenLM/4.2. KenLM'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-gram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/levi/Speech-Course-Lab/Lab/Lab04 - Whisper and KenLM/4.2. KenLM/test.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "LM = './test.arpa'\n",
    "model = kenlm.LanguageModel(LM)\n",
    "print('{0}-gram model'.format(model.order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language modeling is fun .\n",
      "-64.59443664550781\n"
     ]
    }
   ],
   "source": [
    "sentence = 'language modeling is fun .'\n",
    "print(sentence)\n",
    "print(model.score(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(s):\n",
    "    return sum(prob for prob, _, _ in model.full_scores(s))\n",
    "\n",
    "assert (abs(score(sentence) - model.score(sentence)) < 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.4106082916259766 1: language\n",
      "\t\"language\" is an OOV\n",
      "-15.0 2: language modeling\n",
      "\t\"modeling\" is an OOV\n",
      "-23.6878719329834 1: is\n",
      "-2.2966649532318115 1: fun\n",
      "\t\"fun\" is an OOV\n",
      "-21.139057159423828 1: .\n",
      "-0.060235898941755295 2: . </s>\n",
      "\"language\" is an OOV\n",
      "\"modeling\" is an OOV\n",
      "\"fun\" is an OOV\n"
     ]
    }
   ],
   "source": [
    "# Show scores and n-gram matches\n",
    "words = ['<s>'] + sentence.split() + ['</s>']\n",
    "for i, (prob, length, oov) in enumerate(model.full_scores(sentence)):\n",
    "    print('{0} {1}: {2}'.format(prob, length, ' '.join(words[i+2-length:i+2])))\n",
    "    if oov:\n",
    "        print('\\t\"{0}\" is an OOV'.format(words[i+1]))\n",
    "\n",
    "# Find out-of-vocabulary words\n",
    "for w in words:\n",
    "    if not w in model:\n",
    "        print('\"{0}\" is an OOV'.format(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
