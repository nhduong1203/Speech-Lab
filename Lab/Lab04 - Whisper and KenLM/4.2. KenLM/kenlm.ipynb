{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to KenLM\n",
    "\n",
    "Yes, I'm familiar with KenLM, which is an efficient and widely-used statistical language model (LM) that supports large-scale text processing. KenLM is designed to generate and query probabilistic models for sequences of words, and it's often used for speech recognition, machine translation, and natural language processing (NLP) tasks.\n",
    "\n",
    "Here are some key details about KenLM:\n",
    "\n",
    "- Efficient N-gram Language Modeling: KenLM is mainly based on n-grams, where the probability of a word is predicted based on the previous n−1n−1 words. This makes it more efficient than some other methods for certain NLP tasks.\n",
    "\n",
    "- Memory Optimization: It is highly optimized for memory usage and speed, making it useful for large-scale applications where the model size could be several gigabytes. It can work with billions of words and huge datasets without consuming too much memory.\n",
    "\n",
    "- C++ Implementation: KenLM is implemented in C++, which contributes to its speed and efficiency. It also offers Python bindings for easier integration in Python-based NLP pipelines.\n",
    "\n",
    "- Probabilistic Querying: KenLM allows querying the probability of sequences of words, which is critical in applications like decoding speech recognition hypotheses or ranking candidate translations in machine translation systems.\n",
    "\n",
    "- Arpa and Binary Formats: KenLM works with both ARPA format (text-based) and binary format for faster querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install\n",
    "The first step will be to build KenLM. Clone this repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'kenlm'...\n",
      "The authenticity of host 'github.com (20.205.243.166)' can't be established.\n",
      "ED25519 key fingerprint is SHA256:+DiY3wvvV6TuJJhbpZisF/zLDA0zPMSvHdkr4UvCOqU.\n",
      "This key is not known by any other names.\n",
      "Are you sure you want to continue connecting (yes/no/[fingerprint])? ^C\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kpu/kenlm.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install cmake to build KenLM and then following the build guideline from the github repo.\n",
    "```bash\n",
    "sudo snap install cmake\n",
    "mkdir -p build\n",
    "cd build\n",
    "cmake ..\n",
    "make -j 4\n",
    "```\n",
    "After building, you can import kenlm as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "KenLM requires a provided .arpa file to create a language model based on n-grams. The .arpa file is a standardized format that represents an n-gram model, containing word sequences along with their associated probabilities. Each n-gram entry specifies the likelihood of a word following a given sequence of previous words, enabling the language model to make predictions about word sequences.\n",
    "\n",
    "For example, consider the file test.arpa. This file contains metadata about the n-gram model, such as the number of n-grams of each order. In this case, we have 37 unigrams (1-grams), 47 bigrams (2-grams), and so on:\n",
    "```\n",
    "\\data\\\n",
    "ngram 1=37\n",
    "ngram 2=47\n",
    "ngram 3=11\n",
    "ngram 4=6\n",
    "ngram 5=4\n",
    "```\n",
    "\n",
    "Each n-gram entry follows this format: <log-probability> <n-gram sequence> <backoff weight>. In cases where no backoff weight is present, only the log-probability and n-gram sequence are listed.\n",
    "```\n",
    "\\3-grams:\n",
    "-0.01916512\tmore . </s>\n",
    "-0.0283603\ton a little\t-0.4771212\n",
    "-0.0283603\tscreening a little\t-0.4771212\n",
    "-0.01660496\ta little more\t-0.09409451\n",
    "```\n",
    "\n",
    "## Create your own arpa file\n",
    "You can also create your own n-gram language model using a source text. For example, let's build an n-gram model from Wikitext-2, a commonly used dataset in natural language processing. Follow a repository which will build an ARPA file.\n",
    "\n",
    "```\n",
    "git clone git@github.com:daandouwe/ngram-lm.git\n",
    "cd ngram-lm\n",
    "mkdir data\n",
    "./get-data.sh\n",
    "mkdir arpa\n",
    "./main.py --order 3 --interpolate --save-arpa --name wiki-interpolate\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-gram model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /home/levi/Speech-Course-Lab/Lab/Lab04 - Whisper and KenLM/4.2. KenLM/test.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "LM = './test.arpa'\n",
    "model = kenlm.LanguageModel(LM)\n",
    "print('{0}-gram model'.format(model.order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model.score(sentence) function in KenLM returns the log-probability of the sentence according to the loaded language model. \n",
    "- A lower (more negative) score means the sentence is less likely or less common according to the model.\n",
    "- A higher (less negative) score means the sentence is more likely or more common according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking on a little dog\n",
      "-26.229387283325195\n"
     ]
    }
   ],
   "source": [
    "sentence = 'looking on a little dog'\n",
    "print(sentence)\n",
    "print(model.score(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method model.full_scores(sentence) in KenLM provides more detailed information than model.score(sentence). It breaks down the sentence into its component n-grams and provides the following information for each word (or token):\n",
    "- Log-probability of the current word given the previous words.\n",
    "- Backoff weight (the penalty applied when backing off to a lower-order n-gram).\n",
    "- Backoff flag (boolean: True if backoff happened, False if no backoff was needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.48465219140052795, 2, False)\n",
      "(-0.3488368093967438, 3, False)\n",
      "(-0.01552657037973404, 4, False)\n",
      "(-0.0030612230766564608, 5, False)\n",
      "(-4.347817420959473, 1, True)\n",
      "(-21.02949333190918, 1, False)\n"
     ]
    }
   ],
   "source": [
    "for e in model.full_scores(sentence):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.48465219140052795 2: <s> looking\n",
      "-0.3488368093967438 3: <s> looking on\n",
      "-0.01552657037973404 4: <s> looking on a\n",
      "-0.0030612230766564608 5: <s> looking on a little\n",
      "-4.347817420959473 1: dog\n",
      "\t\"dog\" is an OOV\n",
      "-21.02949333190918 1: </s>\n",
      "\"dog\" is an OOV\n"
     ]
    }
   ],
   "source": [
    "# Show scores and n-gram matches\n",
    "words = ['<s>'] + sentence.split() + ['</s>']\n",
    "for i, (prob, length, oov) in enumerate(model.full_scores(sentence)):\n",
    "    print('{0} {1}: {2}'.format(prob, length, ' '.join(words[i+2-length:i+2])))\n",
    "    if oov:\n",
    "        print('\\t\"{0}\" is an OOV'.format(words[i+1]))\n",
    "\n",
    "# Find out-of-vocabulary words\n",
    "for w in words:\n",
    "    if not w in model:\n",
    "        print('\"{0}\" is an OOV'.format(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
