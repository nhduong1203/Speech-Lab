{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper\n",
    "\n",
    "Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al from OpenAI. Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. \n",
    "\n",
    "It has been trained on 680,000 hours of labeled data. This data, which is collected from the internet, is processed carefully to create a large-scale dataset. The method used is called \"Weak Supervision\" because, we use data that already has labels, we do not need to annotate the data ourselves. Instead, we focus on meticulous data processing to ensure its quality.\n",
    "\n",
    "The models were trained on either English-only data or multilingual data. The English-only models were trained on the task of speech recognition. The multilingual models were trained on both speech recognition and speech translation. For speech recognition, the model predicts transcriptions in the same language as the audio. For speech translation, the model predicts transcriptions to a different language to the audio.\n",
    "\n",
    "Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction about Hugging face\n",
    "\n",
    "Hugging Face is a company specializing in natural language processing (NLP) and machine learning. It has gained prominence for its contributions to open-source AI, particularly through its `Transformers` library, which provides pre-trained models for a range of NLP tasks. \n",
    "\n",
    "**Transformers Library:** A popular library that provides state-of-the-art pre-trained models for NLP tasks like text classification, translation, and generation. Provides access to pre-trained models from leading AI research, such as BERT, GPT, and T5. It simplifies fine-tuning and deployment of these models for specific tasks.\n",
    "\n",
    "There several pretrained Whisper model of varying model sizes, you can choose the suitable model for your purpose, visit the openAI site on hugging face for more detail https://huggingface.co/openai. \n",
    "\n",
    "In this lab, we will use `openai/whisper-large-v3`.\n",
    "\n",
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levi/Speech-Course-Lab/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-large-v3\")\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhisperProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `WhisperProcessor` is used to:\n",
    "- Pre-process the audio inputs (converting them to log-Mel spectrograms for the model)\n",
    "- Post-process the model outputs (converting them from tokens to text)\n",
    "\n",
    "Whisper was pretrain on audio with sampling rate of 16000, so we need make sure out audio file have sampling rate of 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5486, -0.5486, -0.5162,  ..., -0.5486, -0.5486, -0.5486],\n",
       "         [-0.5486, -0.4993, -0.3898,  ..., -0.5486, -0.5486, -0.5486],\n",
       "         [-0.4974, -0.4346, -0.1367,  ..., -0.5486, -0.5486, -0.5486],\n",
       "         ...,\n",
       "         [-0.5486, -0.5486,  0.0812,  ..., -0.5486, -0.5486, -0.5486],\n",
       "         [-0.5486, -0.5486,  0.2757,  ..., -0.5486, -0.5486, -0.5486],\n",
       "         [-0.5486, -0.5486,  0.0576,  ..., -0.5486, -0.5486, -0.5486]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Load audio file\n",
    "audio_path = './test.wav'\n",
    "data, sampling_rate = librosa.load(audio_path)\n",
    "\n",
    "# Resampling if sampling_rate != 1600\n",
    "if sampling_rate != 1600:\n",
    "    target_sr = 16000  # Example target sampling rate\n",
    "    y_resampled = librosa.resample(data, orig_sr=sampling_rate, target_sr=target_sr)\n",
    "\n",
    "# WhisperProcessor is used to pre-process the audio input, its output is log-Mel spectrograms of audio.\n",
    "input_features = processor(y_resampled, sampling_rate=target_sr, return_tensors=\"pt\").input_features \n",
    "input_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model determines whether to perform transcription or translation by receiving specific \"context tokens.\" These tokens form a sequence provided to the decoder at the beginning of the decoding process, following this structure:\n",
    "\n",
    "- The transcription starts with the `<|startoftranscript|>` token.\n",
    "- The second token indicates the language (e.g., `<|en|>` for English).\n",
    "- The third token is the \"task token,\" which can either be `<|transcribe|>` for speech recognition or `<|translate|>` for speech translation.\n",
    "- Additionally, a `<|notimestamps|>` token is included if timestamp prediction is not required.\n",
    "\n",
    "For example, this sequence instructs the model to decode in English for the task of speech recognition, without predicting timestamp:\n",
    "\n",
    "```python\n",
    "<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\n",
    "```\n",
    "\n",
    "These tokens can be either forced or unforced. If they are forced, the model must predict each token at every position, allowing control over the output language and task for the Whisper model. If they are unforced, the Whisper model will automatically determine the output language and task.\n",
    "\n",
    "The context tokens can be configured as follows:\n",
    "\n",
    "```python\n",
    "model.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n",
    "```\n",
    "\n",
    "Which forces the model to predict in English under the task of speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Printing, in the only sense with which we are at present concerned, differs from most, if not from all the arts and crafts represented in the exhibition']\n"
     ]
    }
   ],
   "source": [
    "# load model and processor\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context tokens can be removed from the start of the transcription by setting skip_special_tokens=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Printing, in the only sense with which we are at present concerned, differs from most, if not from all the arts and crafts represented in the exhibition']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
